{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import ma\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.interpolate import CloughTocher2DInterpolator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SODA_train_path = 'enso_round1_train_20210201/SODA_train.nc'\n",
    "SODA_train = nc.Dataset(SODA_train_path)\n",
    "SODA_label_path = 'enso_round1_train_20210201/SODA_label.nc'\n",
    "SODA_label = nc.Dataset(SODA_label_path)\n",
    "\n",
    "CMIP_train_path = 'enso_round1_train_20210201/CMIP_train.nc'\n",
    "CMIP_train = nc.Dataset(CMIP_train_path)\n",
    "CMIP_label_path = 'enso_round1_train_20210201/CMIP_label.nc'\n",
    "CMIP_label = nc.Dataset(CMIP_label_path)\n",
    "\n",
    "fea_names = [\"sst\", \"t300\", \"ua\", \"va\"]\n",
    "label_name = \"nino\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_vars = SODA_train.variables.keys()   #获取所有变量名称\n",
    "print(SODA_train.variables)\n",
    "all_vars = SODA_label.variables.keys()   #获取所有变量名称\n",
    "print(SODA_label.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CMIP_train.variables['sst'][0,7,0,:])\n",
    "print(CMIP_train.variables['sst'][0,7,1,:])\n",
    "print(CMIP_train.variables['sst'][0,7,2,:])\n",
    "print(CMIP_train.variables['sst'][0,7,3,:])\n",
    "print(CMIP_train.variables['sst'][0,7,4,:])\n",
    "print(CMIP_train.variables['sst'][0,7,5,:])\n",
    "print(CMIP_train.variables['sst'][0,7,6,:])\n",
    "print(CMIP_train.variables['sst'][0,7,7,:])\n",
    "print(CMIP_train.variables['sst'][0,7,8,:])\n",
    "print(CMIP_train.variables['sst'][0,7,9,:])\n",
    "print(CMIP_train.variables['sst'][0,7,10,:])\n",
    "print(CMIP_train.variables['sst'][0,7,11,:])\n",
    "print(CMIP_train.variables['sst'][0,7,12,:])\n",
    "print(CMIP_train.variables['sst'][0,7,13,:])\n",
    "print(CMIP_train.variables['sst'][0,7,14,:])\n",
    "print(CMIP_train.variables['sst'][0,7,15,:])\n",
    "print(CMIP_train.variables['sst'][0,7,16,:])\n",
    "print(CMIP_train.variables['sst'][0,7,17,:])\n",
    "print(CMIP_train.variables['sst'][0,7,18,:])\n",
    "print(CMIP_train.variables['sst'][0,7,19,:])\n",
    "print(CMIP_train.variables['sst'][0,7,20,:])\n",
    "print(CMIP_train.variables['sst'][0,7,21,:])\n",
    "print(CMIP_train.variables['sst'][0,7,22,:])\n",
    "print(CMIP_train.variables['sst'][0,7,23,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(SODA_train.variables['sst'][0,0,:,:])\n",
    "#print(SODA_train.variables['va'][99,35,:,:])\n",
    "print(SODA_label.variables['nino'][:])\n",
    "soda_label_np = np.zeros(102*12)\n",
    "soda_train_np = np.zeros((4,102*12,24,72))\n",
    "for i, fea in enumerate(fea_names):\n",
    "    soda_train_temp = np.zeros((102*12,24,72))\n",
    "    for j in range(100):\n",
    "        if(j==99):\n",
    "            soda_train_temp[12*j:12*(j+3)] = SODA_train.variables[fea][j, 0:36, :, :]\n",
    "        else:\n",
    "            soda_train_temp[12*j:12*(j+1)] = SODA_train.variables[fea][j, 0:12, :, :]\n",
    "    soda_train_np[i] = soda_train_temp\n",
    "#print(soda_train_np)\n",
    "for j in range(100):\n",
    "    if(j==99):\n",
    "        soda_label_np[12*j:12*(j+3)] = SODA_label.variables['nino'][j, 0:36]\n",
    "    else:\n",
    "        soda_label_np[12*j:12*(j+1)] = SODA_label.variables['nino'][j, 0:12]\n",
    "print(soda_label_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(CMIP_train.variables['sst'][0,0,:,:])\n",
    "#print(CMIP_train.variables['va'][99,35,:,:])\n",
    "#print(CMIP_label.variables['nino'][:])\n",
    "cmip6_label_np = np.zeros((15,153*12))\n",
    "cmip6_train_np = np.zeros((15,4,153*12,24,72))\n",
    "for k in range(15):\n",
    "    #print(CMIP_train.variables['sst'][151*k:151*(k+1),:,:,:])\n",
    "    cmip_train_np = np.zeros((4,153*12,24,72))\n",
    "    for i, fea in enumerate(fea_names):\n",
    "        cmip_train_temp = np.zeros((153*12,24,72))\n",
    "        for j in range(151):\n",
    "            if(j==150):\n",
    "                cmip_train_temp[12*j:12*(j+3)] = CMIP_train.variables[fea][k*151+j, 0:36, :, :]\n",
    "            else:\n",
    "                cmip_train_temp[12*j:12*(j+1)] = CMIP_train.variables[fea][k*151+j, 0:12, :, :]\n",
    "        cmip_train_np[i] = cmip_train_temp\n",
    "    cmip6_train_np[k] = cmip_train_np\n",
    "    #print(cmip_train_np)\n",
    "    \n",
    "    #print(CMIP_label.variables['nino'][151*k:151*(k+1),:])\n",
    "    cmip_label_np = np.zeros(153*12)\n",
    "    for j in range(151):\n",
    "        if(j==150):\n",
    "            cmip_label_np[12*j:12*(j+3)] = CMIP_label.variables['nino'][k*151+j, 0:36]\n",
    "        else:\n",
    "            cmip_label_np[12*j:12*(j+1)] = CMIP_label.variables['nino'][k*151+j, 0:12]\n",
    "    #print(cmip_label_np)\n",
    "    cmip6_label_np[k] = cmip_label_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(CMIP_train.variables['sst'][0,0,:,:])\n",
    "#print(CMIP_train.variables['va'][99,35,:,:])\n",
    "#print(CMIP_label.variables['nino'][:])\n",
    "cmip5_label_np = np.zeros((17,142*12))\n",
    "cmip5_train_np = np.zeros((17,4,142*12,24,72))\n",
    "for k in range(17):\n",
    "    #print(CMIP_train.variables['sst'][2265+140*k:2265+140*(k+1),:,:,:])\n",
    "    cmip_train_np = np.zeros((4,142*12,24,72))\n",
    "    for i, fea in enumerate(fea_names):\n",
    "        cmip_train_temp = np.zeros((142*12,24,72))\n",
    "        for j in range(140):\n",
    "            if(j==139):\n",
    "                cmip_train_temp[12*j:12*(j+3)] = CMIP_train.variables[fea][2265+k*140+j, 0:36, :, :]\n",
    "            else:\n",
    "                cmip_train_temp[12*j:12*(j+1)] = CMIP_train.variables[fea][2265+k*140+j, 0:12, :, :]\n",
    "        cmip_train_np[i] = cmip_train_temp\n",
    "    cmip5_train_np[k] = cmip_train_np\n",
    "    #print(cmip_train_np)\n",
    "    \n",
    "    print(CMIP_label.variables['nino'][2265+140*k:2265+140*(k+1),:])\n",
    "    cmip_label_np = np.zeros(142*12)\n",
    "    for j in range(140):\n",
    "        if(j==139):\n",
    "            cmip_label_np[12*j:12*(j+3)] = CMIP_label.variables['nino'][k*140+j, 0:36]\n",
    "        else:\n",
    "            cmip_label_np[12*j:12*(j+1)] = CMIP_label.variables['nino'][k*140+j, 0:12]\n",
    "    print(cmip_label_np)\n",
    "    cmip5_label_np[k] = cmip_label_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(cmip6_train_np.shape[0]):\n",
    "    for j in range(2):\n",
    "        for k in range(cmip6_train_np.shape[2]):\n",
    "            for l in range(cmip6_train_np.shape[3]):\n",
    "                for m in range(cmip6_train_np.shape[4]):\n",
    "                    if np.isnan(cmip6_train_np[i,j,k,l,m]):\n",
    "                        print('{5}:{0},{1},{2},{3},{4}'.format(i,j,k,l,m,cmip6_train_np[i,j,k,l,m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=100000)\n",
    "print(cmip6_train_np[12,1,1835,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loc = []\n",
    "val = []\n",
    "for i in range(cmip6_train_np.shape[3]):\n",
    "    for j in range(cmip6_train_np.shape[4]):\n",
    "        if(np.isnan(cmip6_train_np[12,1,1835,i,j])==False):\n",
    "            temp = []\n",
    "            temp.append(i)\n",
    "            temp.append(j)\n",
    "            loc.append(temp)\n",
    "            val.append(cmip6_train_np[12,1,1835,i,j])\n",
    "loc = np.array(loc)\n",
    "val = np.array(val)\n",
    "#print(loc)\n",
    "#print(val)\n",
    "x, y = np.mgrid[0:23:24j, 0:71:72j]\n",
    "grid = griddata(loc, val, (x,y), method=\"cubic\",fill_value=0)\n",
    "#grid = CloughTocher2DInterpolator(loc, val)\n",
    "#grid = interp2d(x, y, cmip6_train_np[12,1,1835,:,:], kind='cubic')\n",
    "print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        input_dim: Number of channels in input\n",
    "        hidden_dim: Number of hidden channels\n",
    "        kernel_size: Size of kernel in convolutions\n",
    "        num_layers: Number of LSTM layers stacked on each other\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers\n",
    "        Note: Will do same padding.\n",
    "    Input:\n",
    "        A tensor of size B, T, C, H, W or T, B, C, H, W\n",
    "    Output:\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo\n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            hidden_state = self._init_hidden(batch_size=b,\n",
    "                                             image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
